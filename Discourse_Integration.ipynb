{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lUFN8Of2mj3",
        "outputId": "18b4b9bb-0e63-47ac-fa9d-728f74bcce5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'contrast': [], 'cause': [], 'result': ['therefore'], 'addition': ['also'], 'condition': []}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure you have the necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# List of common discourse markers\n",
        "discourse_markers = {\n",
        "    'contrast': ['but', 'however', 'although', 'yet', 'nevertheless'],\n",
        "    'cause': ['because', 'since', 'due to', 'as'],\n",
        "    'result': ['therefore', 'thus', 'consequently', 'hence'],\n",
        "    'addition': ['and', 'also', 'furthermore', 'moreover'],\n",
        "    'condition': ['if', 'unless', 'provided that']\n",
        "}\n",
        "\n",
        "def categorize_discourse_markers(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    markers_found = {category: [] for category in discourse_markers}\n",
        "\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            for category, markers in discourse_markers.items():\n",
        "                if word in markers:\n",
        "                    markers_found[category].append(word)\n",
        "\n",
        "    return markers_found\n",
        "\n",
        "text = \"We were late; therefore, we missed the opening. But we still enjoyed the show. Also, the food was great.\"\n",
        "markers = categorize_discourse_markers(text)\n",
        "print(markers)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy's English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def hobbs_algorithm(doc):\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'PRON':  # Step 2: Locate the Pronoun\n",
        "            print(f\"Pronoun: {token.text}\")\n",
        "            antecedent = find_antecedent(token)\n",
        "            if antecedent:\n",
        "                print(f\"  Antecedent: {antecedent.text}\")\n",
        "            else:\n",
        "                print(\"  No antecedent found\")\n",
        "\n",
        "def find_antecedent(pronoun):\n",
        "    current_node = pronoun\n",
        "\n",
        "    # Traverse up the tree to find the first NP or S node\n",
        "    while current_node.dep_ not in ('ROOT', 'nsubj', 'dobj', 'pobj'):\n",
        "        current_node = current_node.head\n",
        "\n",
        "    # Traverse down through the left branches of the tree\n",
        "    for child in current_node.lefts:\n",
        "        if child.dep_ in ('nsubj', 'pobj', 'dobj') and child != pronoun:\n",
        "            return child\n",
        "\n",
        "    # Traverse up and look for NP or S in ancestors\n",
        "    for ancestor in current_node.ancestors:\n",
        "        for child in ancestor.lefts:\n",
        "            if child.dep_ in ('nsubj', 'pobj', 'dobj') and child != pronoun:\n",
        "                return child\n",
        "\n",
        "    return None\n",
        "\n",
        "text = \"Samantha found her keys. She was relieved.\"\n",
        "\n",
        "text=text.replace(\".\",\"\").replace(\",\",\"\")\n",
        "\n",
        "doc = nlp(text)\n",
        "hobbs_algorithm(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Z1wbUA4NvQ",
        "outputId": "c861c326-5a6a-4235-e8ff-14d7f7a2a39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pronoun: her\n",
            "  Antecedent: Samantha\n",
            "Pronoun: She\n",
            "  Antecedent: Samantha\n"
          ]
        }
      ]
    }
  ]
}